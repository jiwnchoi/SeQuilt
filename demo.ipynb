{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sequilt import Sequilt\n",
    "from sequilt.model import EventGraph, Sequlet, LabelModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from sequilt.data import get_ids, get_tokenizer\n",
    "\n",
    "# ds = load_dataset(\"neuralbioinfo/bacterial_promoters\")\n",
    "# tokenizer = get_tokenizer(type=\"dna\", k=1)\n",
    "# ids, tokens = get_ids(\n",
    "#   ds[\"test_multispecies\"][\"segment\"], tokenizer, max_tokens=32\n",
    "# )\n",
    "# labels = [\n",
    "#   LabelModel(value=value, name=name)\n",
    "#   for value, name in tokenizer._id_to_token.items()\n",
    "#   if value != 0\n",
    "# ]\n",
    "\n",
    "# labels = sorted(labels, key=lambda x: x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1:\n",
      "  Sentence: 'This is the first paragraph.', Topic: 0\n",
      "  Sentence: 'It contains two sentences.', Topic: 1\n",
      "\n",
      "Paragraph 2:\n",
      "  Sentence: 'This is the second paragraph.', Topic: 0\n",
      "  Sentence: 'It has three sentences.', Topic: 1\n",
      "  Sentence: 'The topic might be different.', Topic: 1\n",
      "\n",
      "Paragraph 3:\n",
      "  Sentence: 'This is the third paragraph.', Topic: 0\n",
      "  Sentence: 'It's about a new topic.', Topic: 1\n",
      "  Sentence: 'It has multiple sentences too.', Topic: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jasonchoi3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# NLTK 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "\n",
    "@dataclass\n",
    "class SentenceTopicModel:\n",
    "    sentence: str\n",
    "    topic: int\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # HTML 태그 제거\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # 특수 문자 제거\n",
    "    text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text: str, stop_words: List[str]) -> str:\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def split_paragraphs_to_sentences(paragraphs: List[str]) -> List[List[str]]:\n",
    "    return [sent_tokenize(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "def perform_topic_modeling(sentences: List[List[str]], num_topics: int, stop_words: List[str]) -> List[List[SentenceTopicModel]]:\n",
    "    # 모든 문장을 하나의 리스트로 평탄화\n",
    "    all_sentences = [sentence for paragraph in sentences for sentence in paragraph]\n",
    "    \n",
    "    # 전처리 및 불용어 제거\n",
    "    preprocessed_sentences = [remove_stopwords(preprocess_text(sentence), stop_words) for sentence in all_sentences]\n",
    "    \n",
    "    # CountVectorizer를 사용하여 텍스트를 벡터화\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
    "    \n",
    "    # LDA 모델 학습\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_output = lda_model.fit_transform(doc_term_matrix)\n",
    "    \n",
    "    # 각 문장에 topic 할당\n",
    "    sentence_topics = [SentenceTopicModel(sentence=sentence, topic=topic.argmax()) \n",
    "                       for sentence, topic in zip(all_sentences, lda_output)]\n",
    "    \n",
    "    # 결과를 원래 구조(List[List[SentenceTopicModel]])로 재구성\n",
    "    result = []\n",
    "    idx = 0\n",
    "    for paragraph in sentences:\n",
    "        paragraph_result = []\n",
    "        for _ in range(len(paragraph)):\n",
    "            paragraph_result.append(sentence_topics[idx])\n",
    "            idx += 1\n",
    "        result.append(paragraph_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def main(paragraphs: List[str], num_topics: int, stop_words: List[str]) -> List[List[SentenceTopicModel]]:\n",
    "    sentences = split_paragraphs_to_sentences(paragraphs)\n",
    "    return perform_topic_modeling(sentences, num_topics, stop_words)\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    paragraphs = [\n",
    "        \"This is the first paragraph. It contains two sentences.\",\n",
    "        \"This is the second paragraph. It has three sentences. The topic might be different.\",\n",
    "        \"This is the third paragraph. It's about a new topic. It has multiple sentences too.\"\n",
    "    ]\n",
    "    num_topics = 2\n",
    "    stop_words = ['is', 'the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of']\n",
    "    \n",
    "    result = main(paragraphs, num_topics, stop_words)\n",
    "    \n",
    "    for i, paragraph in enumerate(result):\n",
    "        print(f\"Paragraph {i + 1}:\")\n",
    "        for sentence in paragraph:\n",
    "            print(f\"  Sentence: '{sentence.sentence}', Topic: {sentence.topic}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: 텍스트 문단 데이터세트 list[str]\n",
    "process:\n",
    "\n",
    "1. 문단을 문장 단위로 나눔 (list[str] -> list[list[str]])\n",
    "2. 문장 단위로 LDA Topic Modeling을 수행함 (Scikit-learn 이용)\n",
    "2-1. 이때, HTML 태그와 Stopwords를 삭제함\n",
    "3. 각 문장에 대해 Topic을 할당함 (list[list[str]] -> list[list[int]])\n",
    "\n",
    "return: \n",
    "아래와 같은 Schmea로 결과를 반환함. (list[list[SentenceTopicModel]])\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class SentenceTopicModel:\n",
    "  sentence: str\n",
    "  topic: int\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2070.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# Language Dataset\n",
    "from datasets import load_dataset\n",
    "from sequilt.data import get_ids, get_tokenizer, get_featured_ids\n",
    "\n",
    "ds = load_dataset(\"ajaykarthick/imdb-movie-reviews\")\n",
    "tokenizer = get_tokenizer(type=\"language\")\n",
    "ids, tokens = get_ids(\n",
    "  ds['test'][\"review\"], tokenizer, max_tokens=32\n",
    ")\n",
    "featured_ids = get_featured_ids(ids, tokenizer, \"tf-idf\", n_features=20)\n",
    "feature_mask = np.isin(ids, featured_ids)\n",
    "ids = np.where(feature_mask, ids, 0)\n",
    "labels = [\n",
    "  LabelModel(value=id, name=tokenizer.id_to_token(id))\n",
    "  for id in featured_ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1295208f1d4911a96165bb2fbd93f4",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Sequilt(labels=[{'value': 59, 'name': 'movie'}, {'value': 5, 'name': 'film'}, {'value': 30, 'name': 'one'}, {'…"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = EventGraph(ids)\n",
    "sequilt = Sequilt(sequence_length=ids.shape[1], n_sequences=ids.shape[0], labels=labels)\n",
    "sequilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event(Position=0, Value=107, # Occurences=93) Event(Position=1, Value=59, # Occurences=545) 18\n",
      "Event(Position=0, Value=59, # Occurences=522) Event(Position=1, Value=112, # Occurences=67) 16\n",
      "Event(Position=0, Value=964, # Occurences=193) Event(Position=1, Value=131, # Occurences=57) 16\n",
      "Event(Position=0, Value=30, # Occurences=249) Event(Position=1, Value=103, # Occurences=47) 13\n",
      "Event(Position=2, Value=964, # Occurences=67) Event(Position=3, Value=131, # Occurences=52) 13\n",
      "Event(Position=1, Value=339, # Occurences=72) Event(Position=2, Value=5, # Occurences=326) 12\n",
      "Event(Position=1, Value=107, # Occurences=50) Event(Position=2, Value=59, # Occurences=468) 11\n",
      "Event(Position=4, Value=964, # Occurences=56) Event(Position=5, Value=131, # Occurences=63) 10\n",
      "Event(Position=0, Value=5, # Occurences=251) Event(Position=1, Value=12, # Occurences=64) 9\n",
      "Event(Position=0, Value=172, # Occurences=60) Event(Position=1, Value=5, # Occurences=308) 9\n",
      "Event(Position=2, Value=103, # Occurences=99) Event(Position=3, Value=107, # Occurences=67) 9\n",
      "Event(Position=3, Value=964, # Occurences=60) Event(Position=4, Value=131, # Occurences=51) 9\n",
      "Event(Position=2, Value=339, # Occurences=37) Event(Position=3, Value=59, # Occurences=334) 8\n",
      "Event(Position=3, Value=12, # Occurences=54) Event(Position=4, Value=59, # Occurences=268) 8\n",
      "Event(Position=5, Value=964, # Occurences=59) Event(Position=6, Value=131, # Occurences=59) 8\n",
      "Event(Position=5, Value=12, # Occurences=55) Event(Position=6, Value=59, # Occurences=184) 8\n",
      "Event(Position=1, Value=385, # Occurences=32) Event(Position=2, Value=104, # Occurences=67) 7\n",
      "Event(Position=3, Value=103, # Occurences=85) Event(Position=4, Value=107, # Occurences=75) 7\n",
      "Event(Position=4, Value=7, # Occurences=58) Event(Position=5, Value=59, # Occurences=227) 7\n",
      "Event(Position=12, Value=7, # Occurences=59) Event(Position=13, Value=59, # Occurences=182) 7\n",
      "Event(Position=22, Value=59, # Occurences=175) Event(Position=23, Value=338, # Occurences=46) 7\n",
      "Event(Position=4, Value=5, # Occurences=228) Event(Position=5, Value=237, # Occurences=51) 6\n",
      "Event(Position=6, Value=964, # Occurences=56) Event(Position=7, Value=131, # Occurences=59) 6\n",
      "Event(Position=7, Value=339, # Occurences=31) Event(Position=8, Value=59, # Occurences=207) 6\n",
      "Event(Position=8, Value=339, # Occurences=39) Event(Position=9, Value=59, # Occurences=209) 6\n",
      "Event(Position=12, Value=103, # Occurences=29) Event(Position=13, Value=30, # Occurences=90) 6\n",
      "Event(Position=14, Value=7, # Occurences=63) Event(Position=15, Value=59, # Occurences=173) 6\n",
      "Event(Position=20, Value=59, # Occurences=162) Event(Position=21, Value=964, # Occurences=49) 6\n",
      "Event(Position=26, Value=964, # Occurences=31) Event(Position=27, Value=131, # Occurences=40) 6\n",
      "Event(Position=10, Value=964, # Occurences=43) Event(Position=11, Value=131, # Occurences=48) 5\n",
      "Event(Position=10, Value=112, # Occurences=45) Event(Position=11, Value=12, # Occurences=39) 5\n",
      "Event(Position=12, Value=59, # Occurences=184) Event(Position=13, Value=12, # Occurences=46) 5\n",
      "Event(Position=13, Value=237, # Occurences=46) Event(Position=14, Value=59, # Occurences=161) 5\n",
      "Event(Position=17, Value=339, # Occurences=56) Event(Position=18, Value=59, # Occurences=180) 5\n",
      "Event(Position=18, Value=112, # Occurences=48) Event(Position=19, Value=7, # Occurences=59) 5\n",
      "Event(Position=18, Value=7, # Occurences=50) Event(Position=19, Value=5, # Occurences=141) 5\n",
      "Event(Position=20, Value=5, # Occurences=139) Event(Position=21, Value=237, # Occurences=32) 5\n",
      "Event(Position=27, Value=7, # Occurences=43) Event(Position=28, Value=5, # Occurences=149) 5\n",
      "Event(Position=30, Value=112, # Occurences=39) Event(Position=31, Value=12, # Occurences=41) 5\n"
     ]
    }
   ],
   "source": [
    "# Draw edges\n",
    "from time import sleep\n",
    "G = EventGraph(ids)\n",
    "sequilt.sequlets = []\n",
    "\n",
    "for id, (event1, event2, cooccurence) in enumerate(G.sorted_edges):\n",
    "  if cooccurence < 5:\n",
    "    break\n",
    "  print(event1, event2, cooccurence)\n",
    "  sequlet = Sequlet(id, [event1, event2])\n",
    "  sleep(0.15)\n",
    "  sequilt.draw_sequlet(sequlet)\n",
    "  G.remove_events_from([event1, event2])\n",
    "  \n",
    "# # Draw nodes\n",
    "# for event in G.events:\n",
    "#   sequlet = Sequlet(len(sequilt), [event])\n",
    "#   sequilt.draw_sequlet(sequlet)\n",
    "#   sleep(0.15)\n",
    "#   # G.remove_event(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
